{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3e39063",
   "metadata": {},
   "source": [
    "# ADS 509 Module 3: Group Comparison - Omar Elfeky\n",
    "\n",
    "The task of comparing two groups of text is fundamental to textual analysis. There are innumerable applications: survey respondents from different segments of customers, speeches by different political parties, words used in Tweets by different constituencies, etc. In this assignment you will build code to effect comparisons between groups of text data, using the ideas learned in reading and lecture.\n",
    "\n",
    "This assignment asks you to analyze the lyrics and Twitter descriptions for the two artists you selected in Module 1. If the results from that pull were not to your liking, you are welcome to use the zipped data from the “Assignment Materials” section. Specifically, you are asked to do the following: \n",
    "\n",
    "* Read in the data, normalize the text, and tokenize it. When you tokenize your Twitter descriptions, keep hashtags and emojis in your token set. \n",
    "* Calculate descriptive statistics on the two sets of lyrics and compare the results. \n",
    "* For each of the four corpora, find the words that are unique to that corpus. \n",
    "* Build word clouds for all four corpora. \n",
    "\n",
    "Each one of the analyses has a section dedicated to it below. Before beginning the analysis there is a section for you to read in the data and do your cleaning (tokenization and normalization). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e65f73",
   "metadata": {},
   "source": [
    "## General Assignment Instructions\n",
    "\n",
    "These instructions are included in every assignment, to remind you of the coding standards for the class. Feel free to delete this cell after reading it. \n",
    "\n",
    "One sign of mature code is conforming to a style guide. We recommend the [Google Python Style Guide](https://google.github.io/styleguide/pyguide.html). If you use a different style guide, please include a cell with a link. \n",
    "\n",
    "Your code should be relatively easy-to-read, sensibly commented, and clean. Writing code is a messy process, so please be sure to edit your final submission. Remove any cells that are not needed or parts of cells that contain unnecessary code. Remove inessential `import` statements and make sure that all such statements are moved into the designated cell. \n",
    "\n",
    "Make use of non-code cells for written commentary. These cells should be grammatical and clearly written. In some of these cells you will have questions to answer. The questions will be marked by a \"Q:\" and will have a corresponding \"A:\" spot for you. *Make sure to answer every question marked with a `Q:` for full credit.* \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abe420bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import emoji\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from wordcloud import WordCloud \n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9f064bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this space for any additional import statements you need\n",
    "from lexicalrichness import LexicalRichness\n",
    "import csv\n",
    "#default is 1e6\n",
    "#jupyter notebook --NotebookApp.iopub_data_rate_limit=1.0e10\n",
    "#for my desktop\n",
    "#os.environ['NUMEXPR_MAX_THREADS'] = '24'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcbe6342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place any addtional functions or constants you need here. \n",
    "\n",
    "# Some punctuation variations\n",
    "punctuation = set(punctuation) # speeds up comparison\n",
    "tw_punct = punctuation - {\"#\"}\n",
    "\n",
    "# Stopwords\n",
    "sw = stopwords.words(\"english\")\n",
    "\n",
    "# Two useful regex\n",
    "whitespace_pattern = re.compile(r\"\\s+\") #re.findall(\"[\\w']+\", text)\n",
    "hashtag_pattern = re.compile(r\"^#[0-9a-zA-Z]+\")\n",
    "\n",
    "# It's handy to have a full set of emojis\n",
    "all_language_emojis = set()\n",
    "\n",
    "for country in emoji.UNICODE_EMOJI : \n",
    "    for em in emoji.UNICODE_EMOJI[country] : \n",
    "        all_language_emojis.add(em)\n",
    "\n",
    "# and now our functions\n",
    "def descriptive_stats(tokens, num_tokens = 5, verbose=True) :\n",
    "    \"\"\"\n",
    "        Given a list of tokens, print number of tokens, number of unique tokens, \n",
    "        number of characters, lexical diversity, and num_tokens most common\n",
    "        tokens. Return a list of \n",
    "    \"\"\"\n",
    "    from collections import Counter\n",
    "\n",
    "    # Fill in the correct values here. \n",
    "    num_tokens = len(tokens)\n",
    "    num_unique_tokens = len(set().union(*tokens)) #set().union(*lyrics_data['tokens'][0])\n",
    "\n",
    "    lexical_diversity = 0.0\n",
    "    # Return Measure of Textual Lexical Diversity (MTLD).\n",
    "    space = \" \"\n",
    "    text = space.join( tokens )\n",
    "    lex = LexicalRichness(text)\n",
    "    #len(tokens)\n",
    "    lexical_diversity = lex.hdd(draws=13) #lex.mtld(threshold=0.69) #lex.Herdan #lex.Summer \n",
    "    num_characters = 0\n",
    "    for token in tokens:\n",
    "        num_characters = num_characters + len(token)\n",
    "\n",
    "    if verbose :        \n",
    "        print(f\"There are {num_tokens} tokens in the data.\")\n",
    "        print(f\"There are {num_unique_tokens} unique tokens in the data.\")\n",
    "        print(f\"There are {num_characters} characters in the data.\")\n",
    "        print(f\"The lexical diversity is {lexical_diversity:.3f} in the data.\")\n",
    "    \n",
    "        # print the five most common tokens\n",
    "        # Pass the split_it list to instance of Counter class.\n",
    "        count1 = Counter(tokens)\n",
    "\n",
    "        # most_common() produces k frequently encountered\n",
    "        # input values and their respective counts.\n",
    "        most_occur = count1.most_common(5)\n",
    "        print(most_occur)\n",
    "  \n",
    "    return([num_tokens, num_unique_tokens,\n",
    "            lexical_diversity,\n",
    "            num_characters])\n",
    "\n",
    "\n",
    "    \n",
    "def is_emoji(s):\n",
    "    return(s in all_language_emojis)\n",
    "\n",
    "def contains_emoji(s):\n",
    "    \n",
    "    s = str(s)\n",
    "    emojis = [ch for ch in s if is_emoji(ch)]\n",
    "\n",
    "    return(len(emojis) > 0)\n",
    "\n",
    "\n",
    "def remove_stopwords(data):\n",
    "    sw = stopwords.words(\"english\")\n",
    "    output_array=[]\n",
    "    for sentence in data:\n",
    "        temp_list=[]\n",
    "        for word in sentence.split():\n",
    "            if word.lower() not in sw:\n",
    "                temp_list.append(word)\n",
    "        output_array.append(' '.join(temp_list))\n",
    "    cleanedArray = list(filter(None, output_array))    \n",
    "    cleanedArray = [x for x in cleanedArray if str(x) != 'nan']\n",
    "    return cleanedArray\n",
    " \n",
    "def remove_punctuation(text, punct_set=tw_punct) : \n",
    "    #punctuation = set(punctuation) # speeds up comparison\n",
    "    punct_set = set(punct_set)\n",
    "    return(\"\".join([ch for ch in text if ch not in punct_set]))\n",
    "\n",
    "def tokenize(text) : \n",
    "    \"\"\" Splitting on whitespace rather than the book's tokenize function. That \n",
    "        function will drop tokens like '#hashtag' or '2A', which we need for Twitter. \"\"\"\n",
    "    \n",
    "    # modify this function to return tokens\n",
    "    #is this function needed? Why don't we just use string.split()?\n",
    "    #also removing \"nulls\"\n",
    "    return( re.split('\\s+', text) )\n",
    "\n",
    "def prepare(text, pipeline) : \n",
    "    tokens = str(text)\n",
    "    \n",
    "    for transform in pipeline : \n",
    "        tokens = transform(tokens)\n",
    "        \n",
    "    return(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47735524",
   "metadata": {},
   "source": [
    "## Data Ingestion\n",
    "\n",
    "Use this section to ingest your data into the data structures you plan to use. Typically this will be a dictionary or a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff88201f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel fre to use the below cells as an example or read in the data in a way you prefer\n",
    "\n",
    "data_location = \"C:/Users/elfek/datamining/\" # change to your location if it is not in the same directory as your notebook\n",
    "twitter_folder = \"twitter/\"\n",
    "lyrics_folder = \"lyrics/\"\n",
    "artist1 = 'glassanimals'\n",
    "artist2 = 'joyner'\n",
    "artist_files = {artist1:'cher_followers_data.txt',\n",
    "                artist2:'robynkonichiwa_followers_data.txt'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df415d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_data = pd.read_csv(data_location + twitter_folder + artist_files[artist1],\n",
    "                           sep=\"\\t\",\n",
    "                           quoting=3)\n",
    "\n",
    "twitter_data['artist'] = artist1\n",
    "twitter_data_2 = pd.read_csv(data_location + twitter_folder + artist_files[artist2],\n",
    "                             sep=\"\\t\",\n",
    "                             quoting=3)\n",
    "twitter_data_2['artist'] = artist2\n",
    "\n",
    "twitter_data = pd.concat([\n",
    "    twitter_data,twitter_data_2])\n",
    "    \n",
    "del(twitter_data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8107c6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "966804cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lyrics</th>\n",
       "      <th>artist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nGlass Animals Lyrics\\n\"Agnes\"\\nAgnes just st...</td>\n",
       "      <td>glassanimals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\nGlass Animals Lyrics\\n\"Black Mambo\"\\nWhat'll...</td>\n",
       "      <td>glassanimals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\nGlass Animals Lyrics\\n\"Cane Shuga\"\\nBaby, do...</td>\n",
       "      <td>glassanimals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nGlass Animals Lyrics\\n\"Cocoa Hooves\"\\nThis o...</td>\n",
       "      <td>glassanimals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\nGlass Animals Lyrics\\n\"Cocoa Hooves, Pt. II\"...</td>\n",
       "      <td>glassanimals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>\\nJoyner Lucas &amp; J. Cole Lyrics\\n\"Your Heart\"\\...</td>\n",
       "      <td>joyner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>\\nJoyner Lucas Lyrics\\n\"Zeze Freestyle\"\\nAhh, ...</td>\n",
       "      <td>joyner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>\\nJoyner Lucas Lyrics\\n\"Zim Zimma\"\\nZim Zimma ...</td>\n",
       "      <td>joyner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>\\nJoyner Lucas &amp; Meek Mill Lyrics\\n\"Run It\"\\n[...</td>\n",
       "      <td>joyner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>\\nRoyce da 5'9\", Joyner Lucas &amp; Black Thought ...</td>\n",
       "      <td>joyner</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>164 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                lyrics        artist\n",
       "0    \\nGlass Animals Lyrics\\n\"Agnes\"\\nAgnes just st...  glassanimals\n",
       "1    \\nGlass Animals Lyrics\\n\"Black Mambo\"\\nWhat'll...  glassanimals\n",
       "2    \\nGlass Animals Lyrics\\n\"Cane Shuga\"\\nBaby, do...  glassanimals\n",
       "3    \\nGlass Animals Lyrics\\n\"Cocoa Hooves\"\\nThis o...  glassanimals\n",
       "4    \\nGlass Animals Lyrics\\n\"Cocoa Hooves, Pt. II\"...  glassanimals\n",
       "..                                                 ...           ...\n",
       "104  \\nJoyner Lucas & J. Cole Lyrics\\n\"Your Heart\"\\...        joyner\n",
       "105  \\nJoyner Lucas Lyrics\\n\"Zeze Freestyle\"\\nAhh, ...        joyner\n",
       "106  \\nJoyner Lucas Lyrics\\n\"Zim Zimma\"\\nZim Zimma ...        joyner\n",
       "107  \\nJoyner Lucas & Meek Mill Lyrics\\n\"Run It\"\\n[...        joyner\n",
       "108  \\nRoyce da 5'9\", Joyner Lucas & Black Thought ...        joyner\n",
       "\n",
       "[164 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in the lyrics here\n",
    "\n",
    "\n",
    "#reading the file\n",
    "lyrics1 = []  \n",
    "os.chdir(r\"C:\\Users\\elfek\\datamining\\lyrics\")\n",
    "for filename in os.listdir(artist1):\n",
    "    with open(os.path.join(artist1, filename), 'r', encoding=\"utf-8\") as f:\n",
    "        songLyrics = f.read()\n",
    "        lyrics1.append(songLyrics.split('\\n', 1)[1:])\n",
    "\n",
    "lyrics2 = []        \n",
    "os.chdir(r\"C:\\Users\\elfek\\datamining\\lyrics\")\n",
    "for filename in os.listdir(artist2):\n",
    "    with open(os.path.join(artist2, filename), 'r', encoding=\"utf-8\") as f:\n",
    "        songLyrics = f.read()\n",
    "        lyrics2.append(songLyrics.split('\\n', 1)[1:]) #extend\n",
    "        #print(songTitle)\n",
    "        \n",
    "\n",
    "lyrics_data =  pd.DataFrame(lyrics1)\n",
    "lyrics_data['artist'] = artist1\n",
    "lyrics_data_2 =  pd.DataFrame(lyrics2)\n",
    "lyrics_data_2['artist'] = artist2\n",
    "\n",
    "lyrics_data = pd.concat([\n",
    "    lyrics_data,lyrics_data_2])\n",
    "    \n",
    "del(lyrics_data_2)\n",
    "\n",
    "lyrics_data.rename(columns={0: \"lyrics\"}, inplace = True)\n",
    "\n",
    "lyrics_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9892d14",
   "metadata": {},
   "source": [
    "## Tokenization and Normalization\n",
    "\n",
    "In this next section, tokenize and normalize your data. We recommend the following cleaning. \n",
    "\n",
    "**Lyrics** \n",
    "\n",
    "* Remove song titles\n",
    "* Casefold to lowercase\n",
    "* Remove punctuation\n",
    "* Split on whitespace\n",
    "* Remove stopwords (optional)\n",
    "\n",
    "Removal of stopwords is up to you. Your descriptive statistic comparison will be different if you include stopwords, though TF-IDF should still find interesting features for you.\n",
    "\n",
    "**Twitter Descriptions** \n",
    "\n",
    "* Casefold to lowercase\n",
    "* Remove punctuation other than emojis or hashtags\n",
    "* Split on whitespace\n",
    "* Remove stopwords\n",
    "\n",
    "Removing stopwords seems sensible for the Twitter description data. Remember to leave in emojis and hashtags, since you analyze those. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca379eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the `pipeline` techniques from BTAP Ch 1 or 5\n",
    "\n",
    "my_pipeline = [str.lower, remove_punctuation, tokenize, remove_stopwords]\n",
    "\n",
    "lyrics_data[\"tokens\"] = lyrics_data[\"lyrics\"].apply(prepare,pipeline=my_pipeline)\n",
    "lyrics_data[\"num_tokens\"] = lyrics_data[\"tokens\"].map(len) \n",
    "\n",
    "twitter_data[\"tokens\"] = twitter_data[\"description\"].apply(prepare,pipeline=my_pipeline)\n",
    "twitter_data[\"num_tokens\"] = twitter_data[\"tokens\"].map(len) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf534be",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_data['has_emoji'] = twitter_data[\"description\"].apply(contains_emoji)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec69ac9",
   "metadata": {},
   "source": [
    "Let's take a quick look at some descriptions with emojis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5a0512",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_data[twitter_data.has_emoji].sample(10)[[\"artist\",\"description\",\"tokens\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2c55c9",
   "metadata": {},
   "source": [
    "With the data processed, we can now start work on the assignment questions. \n",
    "\n",
    "Q: What is one area of improvement to your tokenization that you could theoretically carry out? (No need to actually do it; let's not make perfect the enemy of good enough.)\n",
    "\n",
    "A: We can add other characters such as periods, commas, and forward slashes in addition to the space when extracting tokens. Every sentence at least has a word ending in a period."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1594271",
   "metadata": {},
   "source": [
    "## Calculate descriptive statistics on the two sets of lyrics and compare the results. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc25e937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "#Aggregate the lyrics tokens by artist to be able to get the descriptive statistics.\n",
    "tmpLyricList1 = []\n",
    "for line in lyrics_data.loc[lyrics_data['artist'] == artist1]['tokens']:\n",
    "    tmpLyricList1.extend(line)\n",
    "    \n",
    "tmpLyricList2 = []\n",
    "for line in lyrics_data.loc[lyrics_data['artist'] == artist2]['tokens']:\n",
    "    tmpLyricList2.extend(line)\n",
    "    \n",
    "#Aggregate the twitter description tokens by artist.\n",
    "tmpTwitList1 = []\n",
    "for line in twitter_data.loc[twitter_data['artist'] == artist1]['tokens']:\n",
    "    tmpTwitList1.extend(line)\n",
    "\n",
    "tmpTwitList2 = []\n",
    "for line in twitter_data.loc[twitter_data['artist'] == artist2]['tokens']:\n",
    "    tmpTwitList2.extend(line)\n",
    "    \n",
    "print(\"Lyrics Stats for \" + artist1 + \"\\n\" )\n",
    "print(descriptive_stats(tmpLyricList1) )\n",
    "print(\"\\nLyrics Stats for \" + artist2 + \"\\n\" )\n",
    "print(descriptive_stats(tmpLyricList2) )\n",
    "\n",
    "#Twitter stats not needed\n",
    "# print(\"\\nTwitter Stats for \" + artist1 + \"\\n\" )\n",
    "# print(descriptive_stats(tmpTwitList1) )\n",
    "# print(\"\\nTwitter Stats for \" + artist1 + \"\\n\" )\n",
    "# print(descriptive_stats(tmpTwitList2) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c557882b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a2ada9",
   "metadata": {},
   "source": [
    "Q: what observations do you make about these data? \n",
    "\n",
    "A: Although there are double the number of lyrics for Cher compared to Robyn the stats are very similar. The same lexical diversity and the same top words although the ordering is slightly diff.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750aa526",
   "metadata": {},
   "source": [
    "## Find tokens uniquely related to a corpus\n",
    "\n",
    "Typically we would use TF-IDF to find unique tokens in documents. Unfortunately, we either have too few documents, if we view each data source as a single document, or too many, if we view each description as a separate document. In the latter case, our problem will be that descriptions tend to be short, so our matrix would be too sparse to support analysis. \n",
    "\n",
    "To get around this, we find tokens for each corpus that match the following criteria:\n",
    "\n",
    "1. The token appears at least `n` times in all corpora\n",
    "1. The tokens are in the top 10 for the highest ratio of appearances in a given corpora vs appearances in other corpora.\n",
    "\n",
    "You will choose a cutoff for yourself based on the side of the corpus you're working with. If you're working with the Robyn-Cher corpora provided, `n=5` seems to perform reasonably well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb93dde",
   "metadata": {},
   "source": [
    "Term Frequency (TF)\n",
    "The number of times a word appears in a document divded by the total number of words in the document. Every document has its own term frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eef776f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#these veriations return a dict or a paired list of words and count.\n",
    "def findWords1(bagOfWords, n):\n",
    "    #sorted list of word counts\n",
    "    #wordCountSorted = sorted((Counter(tmpTwitList1).most_common()).items(), key=lambda x : x[1], reverse=True)\n",
    "    wordCountSorted = dict(Counter(tmpTwitList1).most_common())\n",
    "    nCounted = {}\n",
    "    for key, value in wordCountSorted.items():\n",
    "        if (value >= n):\n",
    "            nCounted[key] = value\n",
    "    return nCounted\n",
    "# def findWords2(bagOfWords, n):\n",
    "#     #sorted list of word counts\n",
    "#     #wordCountSorted = sorted((Counter(tmpTwitList1).most_common()).items(), key=lambda x : x[1], reverse=True)\n",
    "#     wordCountSorted = Counter(tmpTwitList1).most_common()\n",
    "#     nCounted = []\n",
    "#     for key, value in wordCountSorted:\n",
    "#         if (value >= n):\n",
    "#             nCounted.append([key, value])\n",
    "#     return nCounted\n",
    "\n",
    "#Find token that occurs at least n times in each corpora.\n",
    "#This only returns the words/tokens\n",
    "def findWords3(bagOfWords, n):\n",
    "    #sorted list of word counts\n",
    "    #wordCountSorted = sorted((Counter(tmpTwitList1).most_common()).items(), key=lambda x : x[1], reverse=True)\n",
    "    wordCountSorted = Counter(tmpTwitList1).most_common()\n",
    "    nCounted = []\n",
    "    for word, count in wordCountSorted:\n",
    "        if (count >= n):\n",
    "            nCounted.append(word)\n",
    "    return nCounted\n",
    "\n",
    "dictnWordsTwit1 = findWords1(tmpTwitList1, 5)\n",
    "dictnWordsTwit2 = findWords1(tmpTwitList2, 5)\n",
    "dictnWordsLyrics1 = findWords1(tmpLyricList1, 5)\n",
    "dictnWordsLyrics2 = findWords1(tmpLyricList2, 5)\n",
    "\n",
    "nWordsTwit1 = findWords3(tmpTwitList1, 5)\n",
    "nWordsTwit2 = findWords3(tmpTwitList2, 5)\n",
    "nWordsLyrics1 = findWords3(tmpLyricList1, 5)\n",
    "nWordsLyrics2 = findWords3(tmpLyricList2, 5)\n",
    "\n",
    "#Get a list of words with >n counts that exist in both corpora\n",
    "#shared_keys = dict_of_dicts_a.keys() & dict_of_dicts_b.keys() # how to combine keys, but not needed\n",
    "uniqueWordsTwitter = set(tmpTwitList1).intersection(set(tmpTwitList2))\n",
    "uniqueWordsLyrics =set(tmpLyricList1).intersection(set(tmpLyricList2))\n",
    "\n",
    "#show top and bottom 10 word counts\n",
    "#This was a test to make sure only words counted n re more times are saved\n",
    "nWordsTwit1 = findWords2(tmpTwitList1, 5)\n",
    "nWordsTwit1[0:9] ,nWordsTwit1[-11:-1] , len(uniqueWordsTwitter), len(uniqueWordsLyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce72f0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Each corpus is a collection of documents. One corpus for all the lyrics for arist1, artist2... Corpora is plural of corporus\n",
    "    \n",
    "def computeTF(wordDict, bagOfWords):\n",
    "    tfDict = {}\n",
    "    bagOfWordsCount = len(bagOfWords)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count / float(bagOfWordsCount)\n",
    "    return tfDict\n",
    "\n",
    "#get frequency of words with >n for artist1's twitter corpora\n",
    "tfTwitterArtist1 = computeTF(dict(Counter(tmpTwitList1).most_common()), tmpTwitList1 )\n",
    "#get frequency of words with >n for artist1's lyrics corpora\n",
    "tfLyricsArtist1 = computeTF(dict(Counter(tmpLyricList1).most_common()),  tmpLyricList1)\n",
    "#get frequency of words with >n for artist1's twitter corpora\n",
    "tfTwitterArtist2 = computeTF(dict(Counter(tmpTwitList2).most_common()), tmpTwitList2 )\n",
    "#get frequency of words with >n for artist1's lyrics corpora\n",
    "tfLyricsArtist2 = computeTF(dict(Counter(tmpLyricList2).most_common()),  tmpLyricList2)\n",
    "#tfTwitterArtist1, tfLyricsArtist1, tfTwitterArtist2, tfLyricsArtist2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d044d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top freq tokens in twitter descriptions for \" + artist1 + \"\\n\" )\n",
    "print(sorted(tfTwitterArtist1.items(), key=lambda x : x[1], reverse=True)[0:10])\n",
    "print(\"\\nTop freq tokens in twitter descriptions for \" + artist2 + \"\\n\" )\n",
    "print(sorted(tfTwitterArtist2.items(), key=lambda x : x[1], reverse=True)[0:10])\n",
    "print(\"\\nTop freq tokens in lyrics for \" + artist1 + \"\\n\" )\n",
    "print(sorted(tfLyricsArtist1.items(), key=lambda x : x[1], reverse=True)[0:10])\n",
    "print(\"\\nTop freq tokens in lyrics for \" + artist2 + \"\\n\" )\n",
    "print(sorted(tfLyricsArtist2.items(), key=lambda x : x[1], reverse=True)[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb30c1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53526fcd",
   "metadata": {},
   "source": [
    "Q: What are some observations about the top tokens? Do you notice any interesting items on the list? \n",
    "\n",
    "A: This provides more insight to the differences between the corpora than the lexical complexity figures. I can see there are some terms that exist in both artists' corpora. I was surprized that a bullet point emoji made it to the top 10 for robyn's followers. I wonder if that has a special meaning to her fans.\n",
    "\n",
    "The first time only used the tokens that existed in both lists that had more than n = 5 occurances for each corpus. This caused all the top 10 words to be the same (but with difference frequencies). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4f52b3",
   "metadata": {},
   "source": [
    "## Build word clouds for all four corpora. \n",
    "\n",
    "For building wordclouds, we'll follow exactly the code of the text. The code in this section can be found [here](https://github.com/blueprints-for-text-analytics-python/blueprints-text/blob/master/ch01/First_Insights.ipynb). If you haven't already, you should absolutely clone the repository that accompanies the book. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786b2af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def wordcloud(word_freq, title=None, max_words=200, stopwords=None):\n",
    "\n",
    "    wc = WordCloud(width=800, height=400, \n",
    "                   background_color= \"black\", colormap=\"Paired\", \n",
    "                   max_font_size=150, max_words=max_words)\n",
    "    \n",
    "    # convert data frame into dict\n",
    "    if type(word_freq) == pd.Series:\n",
    "        counter = Counter(word_freq.fillna(0).to_dict())\n",
    "    else:\n",
    "        counter = word_freq\n",
    "\n",
    "    # filter stop words in frequency counter\n",
    "    if stopwords is not None:\n",
    "        counter = {token:freq for (token, freq) in counter.items() \n",
    "                              if token not in stopwords}\n",
    "    wc.generate_from_frequencies(counter)\n",
    " \n",
    "    plt.title(title) \n",
    "\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    \n",
    "def count_words(df, column='tokens', preprocess=None, min_freq=2):\n",
    "\n",
    "    # process tokens and update counter\n",
    "    def update(doc):\n",
    "        tokens = doc if preprocess is None else preprocess(doc)\n",
    "        counter.update(tokens)\n",
    "\n",
    "    # create counter and run through all data\n",
    "    counter = Counter()\n",
    "    df[column].map(update)\n",
    "\n",
    "    # transform counter into data frame\n",
    "    freq_df = pd.DataFrame.from_dict(counter, orient='index', columns=['freq'])\n",
    "    freq_df = freq_df.query('freq >= @min_freq')\n",
    "    freq_df.index.name = 'token'\n",
    "    \n",
    "    return freq_df.sort_values('freq', ascending=False)\n",
    "\n",
    "#All four corpora\n",
    "# tmpTwitList1\n",
    "# tmpTwitList2\n",
    "# tmpLyricList1\n",
    "# tmpLyricList2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0c3623",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,10))\n",
    "plt.subplot(2,2,1)\n",
    "wordcloud(count_words(lyrics_data.loc[lyrics_data['artist'] == artist1])['freq'], max_words=50)\n",
    "plt.subplot(2,2,2)\n",
    "wordcloud(count_words(twitter_data.loc[twitter_data['artist'] == artist1])['freq'], max_words=50)\n",
    "plt.subplot(2,2,3)\n",
    "wordcloud(count_words(lyrics_data.loc[lyrics_data['artist'] == artist2])['freq'], max_words=50)\n",
    "plt.subplot(2,2,4)\n",
    "wordcloud(count_words(twitter_data.loc[twitter_data['artist'] == artist2])['freq'], max_words=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11a2e53",
   "metadata": {},
   "source": [
    "Q: What observations do you have about these (relatively straightforward) wordclouds? \n",
    "\n",
    "A: A lot of common words and frequencies of such words accross the different artists' corpora. The difference mainly is the ranking of those common words in relation to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00dd708",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
